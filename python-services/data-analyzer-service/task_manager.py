"""
ä»»åŠ¡ç®¡ç†å™¨ - ä½¿ç”¨ Redis æŒä¹…åŒ–ä»»åŠ¡çŠ¶æ€
"""
import json
import redis
from typing import Dict, List, Any, Optional
from datetime import datetime
from loguru import logger

from config import config


class TaskManager:
    """ä»»åŠ¡ç®¡ç†å™¨ - åŸºäº Redis"""
    
    def __init__(self):
        """åˆå§‹åŒ– Redis è¿æ¥"""
        try:
            self.redis_client = redis.Redis(
                host=config.REDIS_HOST,
                port=config.REDIS_PORT,
                db=config.REDIS_DB,
                password=config.REDIS_PASSWORD,
                decode_responses=True
            )
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            logger.info(f"âœ… Redis è¿æ¥æˆåŠŸ: {config.REDIS_HOST}:{config.REDIS_PORT}")
        except Exception as e:
            logger.error(f"âŒ Redis è¿æ¥å¤±è´¥: {e}")
            raise
    
    def create_task(
        self,
        task_id: str,
        dataset_size: int,
        mode: str,
        batch_size: int
    ) -> Dict[str, Any]:
        """
        åˆ›å»ºæ–°ä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
            dataset_size: æ•°æ®é›†å¤§å°
            mode: ä¼˜åŒ–æ¨¡å¼
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            ä»»åŠ¡ä¿¡æ¯
        """
        total_batches = (dataset_size + batch_size - 1) // batch_size
        
        task_data = {
            "task_id": task_id,
            "status": "pending",
            "mode": mode,
            "dataset_size": dataset_size,
            "batch_size": batch_size,
            "total_batches": total_batches,
            "completed_batches": 0,
            "progress": 0.0,
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "error": None,
            "statistics": {},
            "current_batch": 0
        }
        
        # ä¿å­˜åˆ° Redis
        self.redis_client.hset(
            f"task:{task_id}",
            mapping={k: json.dumps(v) if isinstance(v, (dict, list)) else str(v) for k, v in task_data.items()}
        )
        
        # æ·»åŠ åˆ°ä»»åŠ¡åˆ—è¡¨
        self.redis_client.zadd("tasks:all", {task_id: datetime.now().timestamp()})
        
        logger.info(f"âœ… ä»»åŠ¡å·²åˆ›å»º: {task_id} (å…± {total_batches} æ‰¹)")
        
        return task_data
    
    def update_task_status(
        self,
        task_id: str,
        status: str,
        **kwargs
    ):
        """
        æ›´æ–°ä»»åŠ¡çŠ¶æ€
        
        Args:
            task_id: ä»»åŠ¡ID
            status: çŠ¶æ€ (pending, processing, completed, failed)
            **kwargs: å…¶ä»–è¦æ›´æ–°çš„å­—æ®µï¼ˆå¦‚ progress, current_phase, completed_batches ç­‰ï¼‰
        """
        updates = {"status": status}
        updates.update(kwargs)
        
        # æ›´æ–° Redis
        self.redis_client.hset(
            f"task:{task_id}",
            mapping={k: json.dumps(v) if isinstance(v, (dict, list)) else str(v) for k, v in updates.items()}
        )
        
        logger.debug(f"ä»»åŠ¡çŠ¶æ€æ›´æ–°: {task_id} -> {status} {kwargs}")
    
    def update_batch_progress(
        self,
        task_id: str,
        batch_index: int,
        batch_result: Dict[str, Any]
    ):
        """
        æ›´æ–°æ‰¹æ¬¡è¿›åº¦
        
        Args:
            task_id: ä»»åŠ¡ID
            batch_index: æ‰¹æ¬¡ç´¢å¼•
            batch_result: æ‰¹æ¬¡å¤„ç†ç»“æœ
        """
        task = self.get_task(task_id)
        if not task:
            logger.error(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
            return
        
        completed_batches = task["completed_batches"] + 1
        total_batches = task["total_batches"]
        progress = (completed_batches / total_batches) * 100
        
        # ä¿å­˜æ‰¹æ¬¡ç»“æœ
        self.redis_client.hset(
            f"task:{task_id}:batch:{batch_index}",
            mapping={k: json.dumps(v) if isinstance(v, (dict, list)) else str(v) for k, v in batch_result.items()}
        )
        
        # æ›´æ–°ä»»åŠ¡è¿›åº¦
        self.update_task_status(
            task_id,
            status="processing",
            completed_batches=completed_batches,
            progress=round(progress, 2),
            current_batch=batch_index
        )
        
        logger.info(f"ğŸ“Š ä»»åŠ¡è¿›åº¦: {task_id} - {completed_batches}/{total_batches} ({progress:.1f}%)")
    
    def complete_task(
        self,
        task_id: str,
        statistics: Dict[str, Any]
    ):
        """
        å®Œæˆä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
            statistics: ç»Ÿè®¡ä¿¡æ¯
        """
        self.update_task_status(
            task_id,
            status="completed",
            progress=100.0,
            end_time=datetime.now().isoformat(),
            statistics=statistics
        )
        
        logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {task_id}")
    
    def fail_task(
        self,
        task_id: str,
        error: str
    ):
        """
        æ ‡è®°ä»»åŠ¡å¤±è´¥
        
        Args:
            task_id: ä»»åŠ¡ID
            error: é”™è¯¯ä¿¡æ¯
        """
        self.update_task_status(
            task_id,
            status="failed",
            end_time=datetime.now().isoformat(),
            error=error
        )
        
        logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {task_id} - {error}")
    
    def get_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–ä»»åŠ¡ä¿¡æ¯
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            ä»»åŠ¡ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        task_data = self.redis_client.hgetall(f"task:{task_id}")
        
        if not task_data:
            return None
        
        # è§£æ JSON å­—æ®µ
        for key in ["statistics", "error"]:
            if key in task_data and task_data[key]:
                try:
                    task_data[key] = json.loads(task_data[key])
                except:
                    pass
        
        # è½¬æ¢æ•°å€¼å­—æ®µ
        for key in ["dataset_size", "batch_size", "total_batches", "completed_batches", "current_batch"]:
            if key in task_data:
                task_data[key] = int(task_data[key])
        
        if "progress" in task_data:
            task_data["progress"] = float(task_data["progress"])
        
        return task_data
    
    def get_batch_results(self, task_id: str) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            æ‰¹æ¬¡ç»“æœåˆ—è¡¨
        """
        task = self.get_task(task_id)
        if not task:
            return []
        
        results = []
        for i in range(task["total_batches"]):
            batch_data = self.redis_client.hgetall(f"task:{task_id}:batch:{i}")
            if batch_data:
                # è§£æ JSON å­—æ®µ
                for key in ["optimized_samples", "statistics"]:
                    if key in batch_data and batch_data[key]:
                        try:
                            batch_data[key] = json.loads(batch_data[key])
                        except:
                            pass
                results.append(batch_data)
        
        return results
    
    def list_tasks(
        self,
        status: Optional[str] = None,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºä»»åŠ¡
        
        Args:
            status: è¿‡æ»¤çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            ä»»åŠ¡åˆ—è¡¨
        """
        # è·å–æ‰€æœ‰ä»»åŠ¡IDï¼ˆæŒ‰æ—¶é—´å€’åºï¼‰
        task_ids = self.redis_client.zrevrange("tasks:all", 0, limit - 1)
        
        tasks = []
        for task_id in task_ids:
            task = self.get_task(task_id)
            if task:
                if status is None or task.get("status") == status:
                    tasks.append(task)
        
        return tasks
    
    def delete_task(self, task_id: str):
        """
        åˆ é™¤ä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
        """
        task = self.get_task(task_id)
        if not task:
            return
        
        # åˆ é™¤æ‰¹æ¬¡æ•°æ®
        for i in range(task["total_batches"]):
            self.redis_client.delete(f"task:{task_id}:batch:{i}")
        
        # åˆ é™¤ä»»åŠ¡æ•°æ®
        self.redis_client.delete(f"task:{task_id}")
        
        # ä»ä»»åŠ¡åˆ—è¡¨ç§»é™¤
        self.redis_client.zrem("tasks:all", task_id)
        
        logger.info(f"ğŸ—‘ï¸ ä»»åŠ¡å·²åˆ é™¤: {task_id}")
    
    def resume_task(self, task_id: str) -> Optional[int]:
        """
        æ¢å¤ä¸­æ–­çš„ä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„æ‰¹æ¬¡ç´¢å¼•ï¼Œå¦‚æœä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²å®Œæˆè¿”å› None
        """
        task = self.get_task(task_id)
        if not task:
            return None
        
        if task["status"] in ["completed", "failed"]:
            return None
        
        # è¿”å›ä¸‹ä¸€ä¸ªæœªå®Œæˆçš„æ‰¹æ¬¡
        next_batch = task["completed_batches"]
        
        logger.info(f"ğŸ”„ æ¢å¤ä»»åŠ¡: {task_id} - ä»æ‰¹æ¬¡ {next_batch} å¼€å§‹")
        
        return next_batch
