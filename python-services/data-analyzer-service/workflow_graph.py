"""
LangGraph å·¥ä½œæµå›¾
ä½¿ç”¨ LangGraph æ„å»ºæ•°æ®ä¼˜åŒ–çš„å¤šæ™ºèƒ½ä½“å·¥ä½œæµ
"""
from typing import TypedDict, List, Dict, Any, Literal
from langgraph.graph import StateGraph, END
from loguru import logger

from agents.diagnostic_agent import DiagnosticAgent
from agents.optimization_agent import OptimizationAgent
from agents.verification_agent import VerificationAgent
from agents.cleaning_agent import CleaningAgent


class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    # è¾“å…¥
    dataset: List[Dict[str, Any]]
    knowledge_base: List[str]
    optimization_guidance: Dict[str, Any]  # ä¼˜åŒ–æŒ‡å¯¼ï¼ˆå¯é€‰ï¼‰
    
    # æ¨¡å¼é€‰æ‹©
    mode: Literal["auto", "guided"]  # auto=æ ‡æ³¨æµç¨‹ä¼˜åŒ–, guided=æŒ‡å®šä¼˜åŒ–
    
    # è¯Šæ–­ç»“æœ
    sparse_clusters: List[Dict]
    low_quality_samples: List[Dict]
    diagnostic_report: Dict[str, Any]
    
    # ä¼˜åŒ–ç»“æœ
    optimized_samples: List[Dict]
    generated_samples: List[Dict]
    optimization_stats: Dict[str, Any]
    
    # æ ¡éªŒç»“æœ
    verified_dataset: List[Dict]
    verification_stats: Dict[str, Any]
    
    # æœ€ç»ˆè¾“å‡º
    final_dataset: List[Dict]
    pii_cleaned_count: int
    
    # å…ƒæ•°æ®
    iteration_id: int
    errors: List[str]


class DataOptimizationWorkflow:
    """æ•°æ®ä¼˜åŒ–å·¥ä½œæµ"""
    
    def __init__(
        self,
        llm_client,
        embedding_model,
        knowledge_base_manager
    ):
        """
        åˆå§‹åŒ–å·¥ä½œæµ
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            embedding_model: Embedding æ¨¡å‹
            knowledge_base_manager: çŸ¥è¯†åº“ç®¡ç†å™¨
        """
        self.llm_client = llm_client
        self.embedding_model = embedding_model
        self.knowledge_base = knowledge_base_manager
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        self.diagnostic_agent = DiagnosticAgent(embedding_model)
        self.optimization_agent = OptimizationAgent(llm_client)
        self.verification_agent = VerificationAgent(llm_client, knowledge_base_manager)
        self.cleaning_agent = CleaningAgent()
        
        # æ„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµå›¾"""
        workflow = StateGraph(WorkflowState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("mode_selector", self._select_mode)
        workflow.add_node("diagnostic", self._run_diagnostic)
        workflow.add_node("optimization", self._run_optimization)
        workflow.add_node("verification", self._run_verification)
        workflow.add_node("cleaning", self._run_cleaning)
        
        # å®šä¹‰è¾¹
        workflow.set_entry_point("mode_selector")
        
        # æ¨¡å¼é€‰æ‹©åè¿›å…¥è¯Šæ–­
        workflow.add_edge("mode_selector", "diagnostic")
        
        # è¯Šæ–­åè¿›å…¥ä¼˜åŒ–
        workflow.add_edge("diagnostic", "optimization")
        
        # ä¼˜åŒ–åè¿›å…¥æ ¡éªŒ
        workflow.add_edge("optimization", "verification")
        
        # æ ¡éªŒåè¿›å…¥æ¸…æ´—
        workflow.add_edge("verification", "cleaning")
        
        # æ¸…æ´—åç»“æŸ
        workflow.add_edge("cleaning", END)
        
        return workflow.compile()
    
    def _select_mode(self, state: WorkflowState) -> WorkflowState:
        """
        é€‰æ‹©ä¼˜åŒ–æ¨¡å¼
        
        - auto: æ ‡æ³¨æµç¨‹ä¼˜åŒ–ï¼ˆæ— ä¼˜åŒ–æŒ‡å¯¼ï¼‰
        - guided: æŒ‡å®šä¼˜åŒ–ï¼ˆæœ‰ä¼˜åŒ–æŒ‡å¯¼ï¼‰
        """
        has_guidance = state.get("optimization_guidance") is not None
        
        if has_guidance:
            state["mode"] = "guided"
            logger.info("ğŸ¯ æ¨¡å¼: æŒ‡å®šä¼˜åŒ–ï¼ˆæ ¹æ®ä¼˜åŒ–æŒ‡å¯¼æ‰§è¡Œï¼‰")
        else:
            state["mode"] = "auto"
            logger.info("ğŸ¤– æ¨¡å¼: æ ‡æ³¨æµç¨‹ä¼˜åŒ–ï¼ˆè‡ªåŠ¨è¯Šæ–­å’Œä¼˜åŒ–ï¼‰")
        
        return state
    
    def _run_diagnostic(self, state: WorkflowState) -> WorkflowState:
        """
        Module 1: è¯Šæ–­
        
        - auto æ¨¡å¼: å…¨é¢è¯Šæ–­ï¼ˆè¯­ä¹‰åˆ†å¸ƒ + æ¨ç†è´¨é‡ï¼‰
        - guided æ¨¡å¼: æ ¹æ®æŒ‡å¯¼è¯Šæ–­ç‰¹å®šé—®é¢˜
        """
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š Module 1: è¯Šæ–­")
        logger.info("="*60)
        
        dataset = state["dataset"]
        mode = state["mode"]
        
        if mode == "auto":
            # æ ‡æ³¨æµç¨‹ä¼˜åŒ–ï¼šå…¨é¢è¯Šæ–­
            logger.info("æ‰§è¡Œå…¨é¢è¯Šæ–­...")
            result = self.diagnostic_agent.diagnose_full(dataset)
        else:
            # æŒ‡å®šä¼˜åŒ–ï¼šæ ¹æ®æŒ‡å¯¼è¯Šæ–­
            guidance = state["optimization_guidance"]
            logger.info(f"æ ¹æ®ä¼˜åŒ–æŒ‡å¯¼è¯Šæ–­: {guidance.get('focus_areas', [])}")
            result = self.diagnostic_agent.diagnose_guided(dataset, guidance)
        
        state["sparse_clusters"] = result["sparse_clusters"]
        state["low_quality_samples"] = result["low_quality_samples"]
        state["diagnostic_report"] = result["report"]
        
        logger.info(f"âœ… è¯Šæ–­å®Œæˆ:")
        logger.info(f"   - ç¨€ç¼ºèšç±»: {len(state['sparse_clusters'])} ä¸ª")
        logger.info(f"   - ä½è´¨é‡æ ·æœ¬: {len(state['low_quality_samples'])} ä¸ª")
        
        return state
    
    def _run_optimization(self, state: WorkflowState) -> WorkflowState:
        """
        Module 2: ç”Ÿæˆå¢å¼º
        
        - COT é‡å†™ä½è´¨é‡æ ·æœ¬
        - åˆæˆç”Ÿæˆç¨€ç¼ºæ ·æœ¬
        """
        logger.info("\n" + "="*60)
        logger.info("ğŸ”§ Module 2: ç”Ÿæˆå¢å¼º")
        logger.info("="*60)
        
        dataset = state["dataset"]
        low_quality_samples = state["low_quality_samples"]
        sparse_clusters = state["sparse_clusters"]
        mode = state["mode"]
        
        # ä¼˜åŒ–ä½è´¨é‡æ ·æœ¬
        logger.info("ä¼˜åŒ–ä½è´¨é‡æ ·æœ¬ï¼ˆCOT é‡å†™ï¼‰...")
        optimized_result = self.optimization_agent.optimize_samples(
            dataset=dataset,
            low_quality_samples=low_quality_samples,
            mode=mode,
            guidance=state.get("optimization_guidance")
        )
        
        # ç”Ÿæˆç¨€ç¼ºæ ·æœ¬
        logger.info("ç”Ÿæˆç¨€ç¼ºæ ·æœ¬...")
        generated_result = self.optimization_agent.generate_samples(
            sparse_clusters=sparse_clusters,
            mode=mode,
            guidance=state.get("optimization_guidance")
        )
        
        state["optimized_samples"] = optimized_result["samples"]
        state["generated_samples"] = generated_result["samples"]
        state["optimization_stats"] = {
            "optimized_count": optimized_result["count"],
            "generated_count": generated_result["count"],
            "high_quality_kept": optimized_result["high_quality_kept"]
        }
        
        logger.info(f"âœ… ç”Ÿæˆå¢å¼ºå®Œæˆ:")
        logger.info(f"   - ä¼˜åŒ–æ ·æœ¬: {optimized_result['count']}")
        logger.info(f"   - ç”Ÿæˆæ ·æœ¬: {generated_result['count']}")
        logger.info(f"   - ä¿ç•™é«˜è´¨é‡: {optimized_result['high_quality_kept']}")
        
        return state
    
    def _run_verification(self, state: WorkflowState) -> WorkflowState:
        """
        Module 3: RAG æ ¡éªŒ
        
        æ ¡éªŒæ‰€æœ‰ä¼˜åŒ–å’Œç”Ÿæˆçš„æ ·æœ¬
        """
        logger.info("\n" + "="*60)
        logger.info("âœ“ Module 3: RAG æ ¡éªŒ")
        logger.info("="*60)
        
        optimized_samples = state["optimized_samples"]
        generated_samples = state["generated_samples"]
        
        # åˆå¹¶éœ€è¦æ ¡éªŒçš„æ ·æœ¬
        samples_to_verify = optimized_samples + generated_samples
        
        logger.info(f"éœ€è¦æ ¡éªŒçš„æ ·æœ¬: {len(samples_to_verify)}")
        
        if samples_to_verify:
            result = self.verification_agent.verify_batch(samples_to_verify)
            
            state["verified_dataset"] = result["verified_samples"]
            state["verification_stats"] = result["stats"]
            
            logger.info(f"âœ… RAG æ ¡éªŒå®Œæˆ:")
            logger.info(f"   - é€šè¿‡: {result['stats']['passed']}")
            logger.info(f"   - ä¿®æ­£: {result['stats']['corrected']}")
            logger.info(f"   - æ‹’ç»: {result['stats']['rejected']}")
        else:
            state["verified_dataset"] = []
            state["verification_stats"] = {
                "total": 0, "passed": 0, "corrected": 0, "rejected": 0
            }
        
        return state
    
    def _run_cleaning(self, state: WorkflowState) -> WorkflowState:
        """
        Module 4: PII æ¸…æ´—
        
        æ¸…æ´—éšç§ä¿¡æ¯
        """
        logger.info("\n" + "="*60)
        logger.info("ğŸ§¹ Module 4: PII æ¸…æ´—")
        logger.info("="*60)
        
        verified_dataset = state["verified_dataset"]
        
        result = self.cleaning_agent.clean_dataset(verified_dataset)
        
        state["final_dataset"] = result["cleaned_dataset"]
        state["pii_cleaned_count"] = result["cleaned_count"]
        
        logger.info(f"âœ… PII æ¸…æ´—å®Œæˆ: æ¸…æ´—äº† {result['cleaned_count']} ä¸ªæ ·æœ¬")
        
        return state
    
    def run(
        self,
        dataset: List[Dict],
        knowledge_base: List[str] = None,
        optimization_guidance: Dict = None,
        iteration_id: int = 0
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®ä¼˜åŒ–å·¥ä½œæµ
        
        Args:
            dataset: åŸå§‹æ•°æ®é›†
            knowledge_base: çŸ¥è¯†åº“ï¼ˆå¯é€‰ï¼‰
            optimization_guidance: ä¼˜åŒ–æŒ‡å¯¼ï¼ˆå¯é€‰ï¼‰
                - å¦‚æœæä¾›ï¼Œä½¿ç”¨"æŒ‡å®šä¼˜åŒ–"æ¨¡å¼
                - å¦‚æœä¸æä¾›ï¼Œä½¿ç”¨"æ ‡æ³¨æµç¨‹ä¼˜åŒ–"æ¨¡å¼
            iteration_id: è¿­ä»£ç¼–å·
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ å¼€å§‹æ•°æ®ä¼˜åŒ–å·¥ä½œæµ - è¿­ä»£ {iteration_id}")
        logger.info(f"{'='*60}")
        logger.info(f"è¾“å…¥æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        # åŠ è½½çŸ¥è¯†åº“
        if knowledge_base:
            logger.info(f"åŠ è½½çŸ¥è¯†åº“: {len(knowledge_base)} æ¡")
            self.knowledge_base.add_knowledge(knowledge_base)
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state: WorkflowState = {
            "dataset": dataset,
            "knowledge_base": knowledge_base or [],
            "optimization_guidance": optimization_guidance,
            "mode": "auto",
            "sparse_clusters": [],
            "low_quality_samples": [],
            "diagnostic_report": {},
            "optimized_samples": [],
            "generated_samples": [],
            "optimization_stats": {},
            "verified_dataset": [],
            "verification_stats": {},
            "final_dataset": [],
            "pii_cleaned_count": 0,
            "iteration_id": iteration_id,
            "errors": []
        }
        
        # æ‰§è¡Œå·¥ä½œæµ
        final_state = self.graph.invoke(initial_state)
        
        # æ„å»ºç»“æœ
        result = {
            "optimized_dataset": final_state["final_dataset"],
            "statistics": {
                "input_size": len(dataset),
                "output_size": len(final_state["final_dataset"]),
                "mode": final_state["mode"],
                "optimization_stats": final_state["optimization_stats"],
                "verification_stats": final_state["verification_stats"],
                "pii_cleaned_count": final_state["pii_cleaned_count"]
            },
            "diagnostic_report": final_state["diagnostic_report"]
        }
        
        logger.info(f"\n{'='*60}")
        logger.info(f"âœ… å·¥ä½œæµå®Œæˆ!")
        logger.info(f"{'='*60}")
        logger.info(f"è¾“å…¥: {len(dataset)} æ ·æœ¬")
        logger.info(f"è¾“å‡º: {len(final_state['final_dataset'])} æ ·æœ¬")
        logger.info(f"æ¨¡å¼: {final_state['mode']}")
        
        return result
